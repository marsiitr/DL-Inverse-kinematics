{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c88506d",
   "metadata": {},
   "source": [
    "### IMPORTING REQUIRED LIBRARIES AND MODULES :- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84463fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import csv\n",
    "from tensorflow.keras.models import Sequential as seq\n",
    "from tensorflow.keras.layers import Dense as den\n",
    "from tensorflow.keras.callbacks import EarlyStopping,TensorBoard\n",
    "from datetime import datetime\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b0af6",
   "metadata": {},
   "source": [
    "### CREATING DATASET(GENERATING ANGLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2216577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\na1=[]\\nb1=[]\\nx11=-0.25\\n\\np = -1\\nfor i in range(1440):\\n  x11 =x11+0.25\\n  y11=-0.25\\n  for j in range (720):\\n    y11 =y11+0.25\\n    a1.append(x11)\\n    b1.append(y11)\\n\\nx =len(a1)\\ns = np.zeros((x,2), dtype = np.float32)\\nfor i in range(x):\\n  p = p +1\\n  \\n  s[i][0] = a1[p]\\n  s[i][1] = b1[p]\\n  \\n\\n\\nfields = [ 'THETA1' ,'Theta2']\\n\\nfilename = '/content/angles_larger_data.csv'\\nwith open(filename, 'w') as csvfile: \\n  csvwriter = csv.writer(csvfile)\\n  csvwriter.writerow(fields)\\n  csvwriter.writerows(s)\\n\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "a1=[]\n",
    "b1=[]\n",
    "x11=-0.25\n",
    "\n",
    "p = -1\n",
    "for i in range(1440):\n",
    "  x11 =x11+0.25\n",
    "  y11=-0.25\n",
    "  for j in range (720):\n",
    "    y11 =y11+0.25\n",
    "    a1.append(x11)\n",
    "    b1.append(y11)\n",
    "\n",
    "x =len(a1)\n",
    "s = np.zeros((x,2), dtype = np.float32)\n",
    "for i in range(x):\n",
    "  p = p +1\n",
    "  \n",
    "  s[i][0] = a1[p]\n",
    "  s[i][1] = b1[p]\n",
    "  \n",
    "\n",
    "\n",
    "fields = [ 'THETA1' ,'Theta2']\n",
    "\n",
    "filename = '/content/angles_larger_data.csv'\n",
    "with open(filename, 'w') as csvfile: \n",
    "  csvwriter = csv.writer(csvfile)\n",
    "  csvwriter.writerow(fields)\n",
    "  csvwriter.writerows(s)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7c45a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "angle = np.loadtxt('angles_larger_data.csv', delimiter=',', dtype=np.float32 , skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24fa6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "a =math.pi/180\n",
    "\n",
    "def x_data(theta_1,theta_2):\n",
    "  theta1 = a*theta_1\n",
    "  theta2 = a*theta_2\n",
    "  x =  math.cos(theta1) + math.cos(theta1 + theta2)\n",
    "  return float(x)\n",
    "\n",
    "def y_data(theta_1,theta_2):\n",
    "  theta1 = a*theta_1\n",
    "  theta2 = a*theta_2\n",
    "  y = math.sin(theta1) + math.sin(theta1 + theta2)\n",
    "  return float(y)\n",
    "\n",
    "x_1 = np.zeros(1036800)\n",
    "y_1 =np.zeros(1036800)\n",
    "\n",
    "s = np.zeros((1036800,5), dtype = np.float32)\n",
    "\n",
    "for k in range(1036800):\n",
    "  \n",
    "    x_1[k] = x_data(angle[k][0],angle[k][1])\n",
    "    \n",
    "    y_1[k] = y_data(angle[k][0],angle[k][1])\n",
    "\n",
    "p = -1\n",
    "for i in range(1036800):\n",
    "  p = p +1\n",
    "  s[i][0] = p\n",
    "  s[i][1] = x_1[p]\n",
    "  s[i][2] = y_1[p]\n",
    "  s[i][3] = angle[p][0]\n",
    "  s[i][4] = angle[p][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e74200c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "s =tf.constant(s)\n",
    "data1 = s\n",
    "train_input = data1[:836800,1:3]\n",
    "train_output1 = data1[:836800 ,3:4 ]\n",
    "train_output2 = data1[:836800 ,4: ]\n",
    "val_input = data1[836800:936800,1:3]\n",
    "val_output1 = data1[836800:936800 ,3:4 ]\n",
    "val_output2 = data1[836800:936800 ,4: ]\n",
    "test_input = data1[936800:,1:3]\n",
    "test_output1 = data1[936800: ,3:4 ]\n",
    "test_output2 = data1[936800: ,4: ]\n",
    "test_input_short = data1[836870:836880,1:3]\n",
    "test_output_short1 = data1[836870:836880, 3:4]\n",
    "test_output_short2 = data1[836870:836880, 4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5d180",
   "metadata": {},
   "source": [
    "### Defining Neural Net for theta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f4bc7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = seq([den(2 ,activation='relu'),\n",
    "              den(16,activation ='relu'),\n",
    "              den(32 ,activation ='relu'),\n",
    "              den(48 ,activation ='relu'),\n",
    "              den(32 ,activation ='relu'),\n",
    "              den(16 ,activation ='relu'),\n",
    "              den(1,activation ='relu')])\n",
    "model1.compile(optimizer ='adam', loss ='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa42485",
   "metadata": {},
   "source": [
    "### Training and validatating  model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "334048da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 25.6738 - val_loss: 13.3704\n",
      "Epoch 2/250\n",
      "26150/26150 [==============================] - 11s 420us/step - loss: 11.7289 - val_loss: 9.4775\n",
      "Epoch 3/250\n",
      "26150/26150 [==============================] - 11s 417us/step - loss: 8.4380 - val_loss: 6.9879\n",
      "Epoch 4/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 6.9336 - val_loss: 7.0981\n",
      "Epoch 5/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 6.5053 - val_loss: 5.7972\n",
      "Epoch 6/250\n",
      "26150/26150 [==============================] - 10s 400us/step - loss: 6.0892 - val_loss: 5.0476\n",
      "Epoch 7/250\n",
      "26150/26150 [==============================] - 10s 392us/step - loss: 5.9213 - val_loss: 5.6134\n",
      "Epoch 8/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 5.7091 - val_loss: 5.6184\n",
      "Epoch 9/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 5.4896 - val_loss: 4.4825\n",
      "Epoch 10/250\n",
      "26150/26150 [==============================] - 10s 400us/step - loss: 5.3219 - val_loss: 4.9370\n",
      "Epoch 11/250\n",
      "26150/26150 [==============================] - 10s 397us/step - loss: 5.2737 - val_loss: 5.1250\n",
      "Epoch 12/250\n",
      "26150/26150 [==============================] - 11s 416us/step - loss: 5.0863 - val_loss: 4.9565\n",
      "Epoch 13/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 4.9969 - val_loss: 6.1192\n",
      "Epoch 14/250\n",
      "26150/26150 [==============================] - 10s 398us/step - loss: 4.8042 - val_loss: 3.9146\n",
      "Epoch 15/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 4.8171 - val_loss: 3.1513\n",
      "Epoch 16/250\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 4.8193 - val_loss: 4.9485\n",
      "Epoch 17/250\n",
      "26150/26150 [==============================] - 11s 402us/step - loss: 4.7228 - val_loss: 4.0149\n",
      "Epoch 18/250\n",
      "26150/26150 [==============================] - 12s 466us/step - loss: 4.7489 - val_loss: 4.1144\n",
      "Epoch 19/250\n",
      "26150/26150 [==============================] - 11s 428us/step - loss: 4.7603 - val_loss: 4.9047\n",
      "Epoch 20/250\n",
      "26150/26150 [==============================] - 11s 409us/step - loss: 4.6159 - val_loss: 4.8154\n",
      "Epoch 21/250\n",
      "26150/26150 [==============================] - 11s 403us/step - loss: 4.6172 - val_loss: 5.2455\n",
      "Epoch 22/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 4.5770 - val_loss: 3.0874\n",
      "Epoch 23/250\n",
      "26150/26150 [==============================] - 10s 399us/step - loss: 4.4474 - val_loss: 5.4571\n",
      "Epoch 24/250\n",
      "26150/26150 [==============================] - 10s 392us/step - loss: 4.4721 - val_loss: 3.7272\n",
      "Epoch 25/250\n",
      "26150/26150 [==============================] - 11s 402us/step - loss: 4.3041 - val_loss: 6.3457\n",
      "Epoch 26/250\n",
      "26150/26150 [==============================] - 11s 403us/step - loss: 4.4340 - val_loss: 7.2784\n",
      "Epoch 27/250\n",
      "26150/26150 [==============================] - 11s 411us/step - loss: 4.3392 - val_loss: 3.7702\n",
      "Epoch 28/250\n",
      "26150/26150 [==============================] - 11s 402us/step - loss: 4.2814 - val_loss: 4.4289\n",
      "Epoch 29/250\n",
      "26150/26150 [==============================] - 11s 412us/step - loss: 4.2731 - val_loss: 2.9945\n",
      "Epoch 30/250\n",
      "26150/26150 [==============================] - 11s 406us/step - loss: 4.2674 - val_loss: 6.4487\n",
      "Epoch 31/250\n",
      "26150/26150 [==============================] - 11s 405us/step - loss: 4.1533 - val_loss: 4.0846\n",
      "Epoch 32/250\n",
      "26150/26150 [==============================] - 10s 401us/step - loss: 4.1515 - val_loss: 3.4425\n",
      "Epoch 33/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 4.0727 - val_loss: 2.5039\n",
      "Epoch 34/250\n",
      "26150/26150 [==============================] - 10s 377us/step - loss: 4.1739 - val_loss: 4.1008\n",
      "Epoch 35/250\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 4.1280 - val_loss: 3.6250\n",
      "Epoch 36/250\n",
      "26150/26150 [==============================] - 10s 377us/step - loss: 4.0876 - val_loss: 3.7316\n",
      "Epoch 37/250\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 4.1021 - val_loss: 3.0655\n",
      "Epoch 38/250\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 4.0760 - val_loss: 5.0859\n",
      "Epoch 39/250\n",
      "26150/26150 [==============================] - 10s 377us/step - loss: 3.9694 - val_loss: 2.9121\n",
      "Epoch 40/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 4.0037 - val_loss: 4.3709\n",
      "Epoch 41/250\n",
      "26150/26150 [==============================] - 10s 399us/step - loss: 3.9521 - val_loss: 4.9470\n",
      "Epoch 42/250\n",
      "26150/26150 [==============================] - 10s 399us/step - loss: 4.0771 - val_loss: 5.2191\n",
      "Epoch 43/250\n",
      "26150/26150 [==============================] - 11s 402us/step - loss: 4.1039 - val_loss: 3.4052\n",
      "Epoch 44/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 4.0594 - val_loss: 4.7286\n",
      "Epoch 45/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 4.1274 - val_loss: 4.2306\n",
      "Epoch 46/250\n",
      "26150/26150 [==============================] - 10s 391us/step - loss: 4.0022 - val_loss: 5.9087\n",
      "Epoch 47/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 3.9833 - val_loss: 5.6913\n",
      "Epoch 48/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 3.9479 - val_loss: 4.9567\n",
      "Epoch 49/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 3.9257 - val_loss: 3.4577\n",
      "Epoch 50/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.9467 - val_loss: 4.1100\n",
      "Epoch 51/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 3.9160 - val_loss: 4.9899\n",
      "Epoch 52/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 3.8539 - val_loss: 3.0082\n",
      "Epoch 53/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.8555 - val_loss: 3.2419\n",
      "Epoch 54/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 3.8151 - val_loss: 3.7797\n",
      "Epoch 55/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 3.7310 - val_loss: 3.3831\n",
      "Epoch 56/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.7365 - val_loss: 4.6269\n",
      "Epoch 57/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.6100 - val_loss: 3.9577\n",
      "Epoch 58/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.5758 - val_loss: 4.6603\n",
      "Epoch 59/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.5682 - val_loss: 3.2456\n",
      "Epoch 60/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.4846 - val_loss: 3.0559\n",
      "Epoch 61/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.5411 - val_loss: 3.2342\n",
      "Epoch 62/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.5622 - val_loss: 3.0027\n",
      "Epoch 63/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.4699 - val_loss: 2.9755\n",
      "Epoch 64/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.3697 - val_loss: 3.5372\n",
      "Epoch 65/250\n",
      "26150/26150 [==============================] - 10s 393us/step - loss: 3.3535 - val_loss: 3.7921\n",
      "Epoch 66/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.4054 - val_loss: 2.1255\n",
      "Epoch 67/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.3962 - val_loss: 2.3990\n",
      "Epoch 68/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.3605 - val_loss: 3.9702\n",
      "Epoch 69/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.2717 - val_loss: 3.2755\n",
      "Epoch 70/250\n",
      "26150/26150 [==============================] - 10s 393us/step - loss: 3.3942 - val_loss: 3.9688\n",
      "Epoch 71/250\n",
      "26150/26150 [==============================] - 10s 396us/step - loss: 3.3468 - val_loss: 5.9536\n",
      "Epoch 72/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3639 - val_loss: 3.4733\n",
      "Epoch 73/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.3883 - val_loss: 4.0779\n",
      "Epoch 74/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 3.4979 - val_loss: 1.5036\n",
      "Epoch 75/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.4118 - val_loss: 3.7728\n",
      "Epoch 76/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3447 - val_loss: 4.1155\n",
      "Epoch 77/250\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 3.4062 - val_loss: 2.9691\n",
      "Epoch 78/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.3378 - val_loss: 3.4182\n",
      "Epoch 79/250\n",
      "26150/26150 [==============================] - 10s 396us/step - loss: 3.4972 - val_loss: 3.9368\n",
      "Epoch 80/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3962 - val_loss: 5.0889\n",
      "Epoch 81/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3748 - val_loss: 4.6590\n",
      "Epoch 82/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3113 - val_loss: 3.7214\n",
      "Epoch 83/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.3463 - val_loss: 2.5801\n",
      "Epoch 84/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.3256 - val_loss: 3.6366\n",
      "Epoch 85/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.3142 - val_loss: 2.9740\n",
      "Epoch 86/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.3742 - val_loss: 3.7372\n",
      "Epoch 87/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3235 - val_loss: 3.3575\n",
      "Epoch 88/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.2279 - val_loss: 3.7457\n",
      "Epoch 89/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.3138 - val_loss: 2.2550\n",
      "Epoch 90/250\n",
      "26150/26150 [==============================] - 10s 394us/step - loss: 3.2769 - val_loss: 3.2464\n",
      "Epoch 91/250\n",
      "26150/26150 [==============================] - 10s 396us/step - loss: 3.3027 - val_loss: 2.2784\n",
      "Epoch 92/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.3665 - val_loss: 4.4028\n",
      "Epoch 93/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.3083 - val_loss: 1.9954\n",
      "Epoch 94/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.3170 - val_loss: 4.5089\n",
      "Epoch 95/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.3953 - val_loss: 2.1580\n",
      "Epoch 96/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.2784 - val_loss: 2.6076\n",
      "Epoch 97/250\n",
      "26150/26150 [==============================] - 11s 403us/step - loss: 3.2602 - val_loss: 1.9501\n",
      "Epoch 98/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.2300 - val_loss: 4.9828\n",
      "Epoch 99/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 3.2286 - val_loss: 2.4620\n",
      "Epoch 100/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.2731 - val_loss: 4.5828\n",
      "Epoch 101/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.2802 - val_loss: 4.5252\n",
      "Epoch 102/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1911 - val_loss: 2.8079\n",
      "Epoch 103/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.2272 - val_loss: 2.1173\n",
      "Epoch 104/250\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 3.2681 - val_loss: 4.6353\n",
      "Epoch 105/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.2659 - val_loss: 2.7272\n",
      "Epoch 106/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3164 - val_loss: 3.1852\n",
      "Epoch 107/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.3297 - val_loss: 2.9393\n",
      "Epoch 108/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.2116 - val_loss: 3.5868\n",
      "Epoch 109/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3592 - val_loss: 2.5121\n",
      "Epoch 110/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.2345 - val_loss: 4.4089\n",
      "Epoch 111/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.2730 - val_loss: 2.4039\n",
      "Epoch 112/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.1648 - val_loss: 3.2375\n",
      "Epoch 113/250\n",
      "26150/26150 [==============================] - 10s 395us/step - loss: 3.3783 - val_loss: 5.1392\n",
      "Epoch 114/250\n",
      "26150/26150 [==============================] - 10s 389us/step - loss: 3.2233 - val_loss: 4.1243\n",
      "Epoch 115/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.2646 - val_loss: 3.6632\n",
      "Epoch 116/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.2089 - val_loss: 2.2669\n",
      "Epoch 117/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.2833 - val_loss: 2.9197\n",
      "Epoch 118/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 3.2807 - val_loss: 3.9107\n",
      "Epoch 119/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.2403 - val_loss: 3.6992\n",
      "Epoch 120/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.2369 - val_loss: 3.5296\n",
      "Epoch 121/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.3530 - val_loss: 3.9167\n",
      "Epoch 122/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.2678 - val_loss: 2.5176\n",
      "Epoch 123/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.2753 - val_loss: 5.0533\n",
      "Epoch 124/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.4104 - val_loss: 4.8670\n",
      "Epoch 125/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.2209 - val_loss: 1.8003\n",
      "Epoch 126/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.1844 - val_loss: 2.2732\n",
      "Epoch 127/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.2224 - val_loss: 2.5707\n",
      "Epoch 128/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.2724 - val_loss: 3.2517\n",
      "Epoch 129/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3464 - val_loss: 2.5603\n",
      "Epoch 130/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1650 - val_loss: 2.5039\n",
      "Epoch 131/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.2618 - val_loss: 3.1126\n",
      "Epoch 132/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.2640 - val_loss: 2.3875\n",
      "Epoch 133/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.2414 - val_loss: 2.0354\n",
      "Epoch 134/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.3300 - val_loss: 3.3292\n",
      "Epoch 135/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1870 - val_loss: 4.6517\n",
      "Epoch 136/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.2293 - val_loss: 4.0776\n",
      "Epoch 137/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.2360 - val_loss: 2.3365\n",
      "Epoch 138/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.2694 - val_loss: 4.9766\n",
      "Epoch 139/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1471 - val_loss: 3.7970\n",
      "Epoch 140/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1343 - val_loss: 2.4910\n",
      "Epoch 141/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.2018 - val_loss: 3.7743\n",
      "Epoch 142/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.2775 - val_loss: 2.7707\n",
      "Epoch 143/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1198 - val_loss: 3.7568\n",
      "Epoch 144/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.2082 - val_loss: 3.3847\n",
      "Epoch 145/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.3481 - val_loss: 2.9548\n",
      "Epoch 146/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1783 - val_loss: 4.5870\n",
      "Epoch 147/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.0371 - val_loss: 1.9784\n",
      "Epoch 148/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1256 - val_loss: 3.6862\n",
      "Epoch 149/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 3.0907 - val_loss: 2.1498\n",
      "Epoch 150/250\n",
      "26150/26150 [==============================] - 11s 402us/step - loss: 3.0906 - val_loss: 3.2688\n",
      "Epoch 151/250\n",
      "26150/26150 [==============================] - 11s 404us/step - loss: 3.2373 - val_loss: 2.4328\n",
      "Epoch 152/250\n",
      "26150/26150 [==============================] - 10s 391us/step - loss: 3.1170 - val_loss: 3.4818\n",
      "Epoch 153/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1405 - val_loss: 2.2657\n",
      "Epoch 154/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.0827 - val_loss: 3.5636\n",
      "Epoch 155/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1873 - val_loss: 2.7986\n",
      "Epoch 156/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.1080 - val_loss: 3.9651\n",
      "Epoch 157/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.2444 - val_loss: 5.2750\n",
      "Epoch 158/250\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 3.1794 - val_loss: 2.8364\n",
      "Epoch 159/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1541 - val_loss: 4.8185\n",
      "Epoch 160/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1224 - val_loss: 3.7648\n",
      "Epoch 161/250\n",
      "26150/26150 [==============================] - 10s 389us/step - loss: 3.1804 - val_loss: 3.1357\n",
      "Epoch 162/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1563 - val_loss: 3.2845\n",
      "Epoch 163/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1371 - val_loss: 4.3775\n",
      "Epoch 164/250\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 3.1656 - val_loss: 3.5332\n",
      "Epoch 165/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1122 - val_loss: 4.5651\n",
      "Epoch 166/250\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 3.0357 - val_loss: 2.6388\n",
      "Epoch 167/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.0672 - val_loss: 3.2756\n",
      "Epoch 168/250\n",
      "26150/26150 [==============================] - 10s 389us/step - loss: 3.0615 - val_loss: 1.7340\n",
      "Epoch 169/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.0943 - val_loss: 2.5092\n",
      "Epoch 170/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.0698 - val_loss: 4.8394\n",
      "Epoch 171/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 2.9997 - val_loss: 1.3860\n",
      "Epoch 172/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 2.9937 - val_loss: 2.6897\n",
      "Epoch 173/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 2.9726 - val_loss: 4.8309\n",
      "Epoch 174/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 2.9611 - val_loss: 4.0581\n",
      "Epoch 175/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1000 - val_loss: 2.5676\n",
      "Epoch 176/250\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 3.0633 - val_loss: 3.9187\n",
      "Epoch 177/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1109 - val_loss: 2.0347\n",
      "Epoch 178/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.0956 - val_loss: 2.8460\n",
      "Epoch 179/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.0794 - val_loss: 4.2371\n",
      "Epoch 180/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.1706 - val_loss: 2.2049\n",
      "Epoch 181/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1719 - val_loss: 3.1117\n",
      "Epoch 182/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.0433 - val_loss: 4.9119\n",
      "Epoch 183/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1550 - val_loss: 3.6413\n",
      "Epoch 184/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1914 - val_loss: 2.4823\n",
      "Epoch 185/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1098 - val_loss: 2.5777\n",
      "Epoch 186/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.1196 - val_loss: 2.6395\n",
      "Epoch 187/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.0653 - val_loss: 1.9554\n",
      "Epoch 188/250\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 3.0583 - val_loss: 3.0735\n",
      "Epoch 189/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.0525 - val_loss: 1.7323\n",
      "Epoch 190/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.0344 - val_loss: 2.6886\n",
      "Epoch 191/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1703 - val_loss: 8.7216\n",
      "Epoch 192/250\n",
      "26150/26150 [==============================] - 10s 393us/step - loss: 3.0342 - val_loss: 3.0532\n",
      "Epoch 193/250\n",
      "26150/26150 [==============================] - 11s 407us/step - loss: 3.1188 - val_loss: 5.1176\n",
      "Epoch 194/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1336 - val_loss: 2.1494\n",
      "Epoch 195/250\n",
      "26150/26150 [==============================] - 10s 390us/step - loss: 3.0147 - val_loss: 4.0534\n",
      "Epoch 196/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.0323 - val_loss: 3.0878\n",
      "Epoch 197/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.0561 - val_loss: 4.0043\n",
      "Epoch 198/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.0560 - val_loss: 1.5412\n",
      "Epoch 199/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 2.9779 - val_loss: 3.1697\n",
      "Epoch 200/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.0874 - val_loss: 2.4817\n",
      "Epoch 201/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.0790 - val_loss: 1.8934\n",
      "Epoch 202/250\n",
      "26150/26150 [==============================] - 10s 400us/step - loss: 3.1031 - val_loss: 3.4240\n",
      "Epoch 203/250\n",
      "26150/26150 [==============================] - 11s 418us/step - loss: 3.0278 - val_loss: 3.5699\n",
      "Epoch 204/250\n",
      "26150/26150 [==============================] - 11s 408us/step - loss: 2.9782 - val_loss: 3.7923\n",
      "Epoch 205/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 2.9714 - val_loss: 1.8108\n",
      "Epoch 206/250\n",
      "26150/26150 [==============================] - 10s 389us/step - loss: 3.0935 - val_loss: 3.2229\n",
      "Epoch 207/250\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 3.1229 - val_loss: 2.9286\n",
      "Epoch 208/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.0013 - val_loss: 3.4230\n",
      "Epoch 209/250\n",
      "26150/26150 [==============================] - 11s 404us/step - loss: 3.1704 - val_loss: 2.2445\n",
      "Epoch 210/250\n",
      "26150/26150 [==============================] - 11s 402us/step - loss: 2.9297 - val_loss: 2.6258\n",
      "Epoch 211/250\n",
      "26150/26150 [==============================] - 10s 398us/step - loss: 3.0096 - val_loss: 5.5378\n",
      "Epoch 212/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 2.9467 - val_loss: 2.6332\n",
      "Epoch 213/250\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 3.1917 - val_loss: 2.7347\n",
      "Epoch 214/250\n",
      "26150/26150 [==============================] - 10s 389us/step - loss: 3.0485 - val_loss: 3.9823\n",
      "Epoch 215/250\n",
      "26150/26150 [==============================] - 10s 394us/step - loss: 2.9936 - val_loss: 2.5682\n",
      "Epoch 216/250\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 3.1188 - val_loss: 2.5218\n",
      "Epoch 217/250\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 3.0528 - val_loss: 3.0791\n",
      "Epoch 218/250\n",
      "26150/26150 [==============================] - 11s 416us/step - loss: 3.0531 - val_loss: 2.0528\n",
      "Epoch 219/250\n",
      "26150/26150 [==============================] - 11s 439us/step - loss: 3.0210 - val_loss: 2.2237\n",
      "Epoch 220/250\n",
      "26150/26150 [==============================] - 12s 443us/step - loss: 3.0698 - val_loss: 3.3895\n",
      "Epoch 221/250\n",
      "26150/26150 [==============================] - 12s 446us/step - loss: 3.0358 - val_loss: 2.8575\n",
      "Epoch 222/250\n",
      "26150/26150 [==============================] - 12s 441us/step - loss: 3.0564 - val_loss: 3.8317\n",
      "Epoch 223/250\n",
      "26150/26150 [==============================] - 12s 441us/step - loss: 3.1063 - val_loss: 3.7631\n",
      "Epoch 224/250\n",
      "26150/26150 [==============================] - 11s 438us/step - loss: 2.9427 - val_loss: 2.4598\n",
      "Epoch 225/250\n",
      "26150/26150 [==============================] - 11s 436us/step - loss: 2.9954 - val_loss: 2.5757\n",
      "Epoch 226/250\n",
      "26150/26150 [==============================] - 11s 426us/step - loss: 2.9105 - val_loss: 1.9682\n",
      "Epoch 227/250\n",
      "26150/26150 [==============================] - 11s 429us/step - loss: 3.0282 - val_loss: 3.7699\n",
      "Epoch 228/250\n",
      "26150/26150 [==============================] - 11s 424us/step - loss: 2.9720 - val_loss: 3.9041\n",
      "Epoch 229/250\n",
      "26150/26150 [==============================] - 11s 424us/step - loss: 2.9987 - val_loss: 2.3137\n",
      "Epoch 230/250\n",
      "26150/26150 [==============================] - 11s 428us/step - loss: 3.0690 - val_loss: 2.3313\n",
      "Epoch 231/250\n",
      "26150/26150 [==============================] - 11s 421us/step - loss: 2.9357 - val_loss: 3.3401\n",
      "Epoch 232/250\n",
      "26150/26150 [==============================] - 11s 420us/step - loss: 3.0391 - val_loss: 2.4184\n",
      "Epoch 233/250\n",
      "26150/26150 [==============================] - 11s 429us/step - loss: 2.9812 - val_loss: 4.0367\n",
      "Epoch 234/250\n",
      "26150/26150 [==============================] - 11s 426us/step - loss: 2.9905 - val_loss: 4.4525\n",
      "Epoch 235/250\n",
      "26150/26150 [==============================] - 11s 417us/step - loss: 3.0114 - val_loss: 4.8218\n",
      "Epoch 236/250\n",
      "26150/26150 [==============================] - 11s 423us/step - loss: 3.0470 - val_loss: 2.4381\n",
      "Epoch 237/250\n",
      "26150/26150 [==============================] - 11s 436us/step - loss: 2.8937 - val_loss: 1.8147\n",
      "Epoch 238/250\n",
      "26150/26150 [==============================] - 11s 423us/step - loss: 3.0268 - val_loss: 2.0207\n",
      "Epoch 239/250\n",
      "26150/26150 [==============================] - 11s 425us/step - loss: 2.9969 - val_loss: 2.3103\n",
      "Epoch 240/250\n",
      "26150/26150 [==============================] - 11s 430us/step - loss: 2.9677 - val_loss: 5.4840\n",
      "Epoch 241/250\n",
      "26150/26150 [==============================] - 12s 451us/step - loss: 3.0669 - val_loss: 1.3847\n",
      "Epoch 242/250\n",
      "26150/26150 [==============================] - 12s 444us/step - loss: 2.9794 - val_loss: 4.0139\n",
      "Epoch 243/250\n",
      "26150/26150 [==============================] - 12s 448us/step - loss: 3.0068 - val_loss: 3.9891\n",
      "Epoch 244/250\n",
      "26150/26150 [==============================] - 12s 450us/step - loss: 2.9300 - val_loss: 2.6856\n",
      "Epoch 245/250\n",
      "26150/26150 [==============================] - 12s 452us/step - loss: 2.9900 - val_loss: 2.4235\n",
      "Epoch 246/250\n",
      "26150/26150 [==============================] - 12s 445us/step - loss: 2.9674 - val_loss: 4.1402\n",
      "Epoch 247/250\n",
      "26150/26150 [==============================] - 12s 449us/step - loss: 3.0337 - val_loss: 3.6588\n",
      "Epoch 248/250\n",
      "26150/26150 [==============================] - 12s 444us/step - loss: 2.9966 - val_loss: 1.7331\n",
      "Epoch 249/250\n",
      "26150/26150 [==============================] - 12s 450us/step - loss: 2.9542 - val_loss: 2.8734\n",
      "Epoch 250/250\n",
      "26150/26150 [==============================] - 11s 438us/step - loss: 2.9468 - val_loss: 3.5981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x253f04a5850>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_input, train_output1,\n",
    "          validation_data=(val_input,val_output1) ,epochs= 250 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de4ea0",
   "metadata": {},
   "source": [
    "#### Evaluating the performance of model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a4565f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 1s 318us/step - loss: 3.5363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.536292552947998"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(test_input, test_output1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0826a943",
   "metadata": {},
   "source": [
    "#### predicting and comparing with a small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6fd550fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[174.65665 ]\n",
      " [ 59.559933]\n",
      " [166.38606 ]\n",
      " [305.62775 ]\n",
      " [209.93216 ]\n",
      " [352.89166 ]\n",
      " [ 62.80483 ]\n",
      " [166.91739 ]\n",
      " [222.78976 ]\n",
      " [ 86.70197 ]]\n",
      "tf.Tensor(\n",
      "[[174.5 ]\n",
      " [ 60.75]\n",
      " [166.  ]\n",
      " [308.  ]\n",
      " [209.5 ]\n",
      " [359.75]\n",
      " [ 73.25]\n",
      " [166.5 ]\n",
      " [223.  ]\n",
      " [ 88.  ]], shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test = model1.predict(test_input_short)\n",
    "print(test)\n",
    "print(test_output_short1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91daffe4",
   "metadata": {},
   "source": [
    "### Training and validatating  model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3817881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = seq([den(2 ,activation='relu'),\n",
    "              den(16,activation ='relu'),\n",
    "              den(32 ,activation ='relu'),\n",
    "              den(32 ,activation ='relu'),\n",
    "              den(32 ,activation ='relu'),\n",
    "              den(16 ,activation ='relu'),\n",
    "              den(1,activation ='relu')])\n",
    "model2.compile(optimizer ='adam', loss ='mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6e62ef",
   "metadata": {},
   "source": [
    "#### Training and validating model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e6f1c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "    2/26150 [..............................] - ETA: 2:12:38 - loss: 89.3838WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.6077s). Check your callbacks.\n",
      "26150/26150 [==============================] - 11s 414us/step - loss: 30.7529 - val_loss: 30.2053\n",
      "Epoch 2/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 7.4089 - val_loss: 1.4738\n",
      "Epoch 3/120\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 1.7643 - val_loss: 1.2030\n",
      "Epoch 4/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 1.5064 - val_loss: 1.4539\n",
      "Epoch 5/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 1.3871 - val_loss: 1.0883\n",
      "Epoch 6/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 1.3138 - val_loss: 1.5101\n",
      "Epoch 7/120\n",
      "26150/26150 [==============================] - 10s 377us/step - loss: 1.2572 - val_loss: 0.9605\n",
      "Epoch 8/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 1.2150 - val_loss: 1.2971\n",
      "Epoch 9/120\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 1.1819 - val_loss: 1.3228\n",
      "Epoch 10/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 1.1575 - val_loss: 1.0868\n",
      "Epoch 11/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 1.1291 - val_loss: 0.9747\n",
      "Epoch 12/120\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 1.0968 - val_loss: 1.4862\n",
      "Epoch 13/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 1.0848 - val_loss: 0.8004\n",
      "Epoch 14/120\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 1.0485 - val_loss: 1.4275\n",
      "Epoch 15/120\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 1.0117 - val_loss: 0.8234\n",
      "Epoch 16/120\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 0.9803 - val_loss: 1.0461\n",
      "Epoch 17/120\n",
      "26150/26150 [==============================] - 10s 377us/step - loss: 0.9581 - val_loss: 0.6282\n",
      "Epoch 18/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.9368 - val_loss: 0.9308\n",
      "Epoch 19/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.9121 - val_loss: 1.0829\n",
      "Epoch 20/120\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 0.8948 - val_loss: 1.2105\n",
      "Epoch 21/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.8829 - val_loss: 0.9092\n",
      "Epoch 22/120\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 0.8721 - val_loss: 0.9548\n",
      "Epoch 23/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 0.8583 - val_loss: 0.6177\n",
      "Epoch 24/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.8524 - val_loss: 0.8892\n",
      "Epoch 25/120\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 0.8402 - val_loss: 1.0824\n",
      "Epoch 26/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.8305 - val_loss: 0.7020\n",
      "Epoch 27/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.8202 - val_loss: 0.5975\n",
      "Epoch 28/120\n",
      "26150/26150 [==============================] - 10s 378us/step - loss: 0.8118 - val_loss: 1.3148\n",
      "Epoch 29/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.8058 - val_loss: 0.7877\n",
      "Epoch 30/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 0.7916 - val_loss: 1.2045\n",
      "Epoch 31/120\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 0.7828 - val_loss: 0.9924\n",
      "Epoch 32/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.7778 - val_loss: 0.9584\n",
      "Epoch 33/120\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 0.7655 - val_loss: 0.5270\n",
      "Epoch 34/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.7614 - val_loss: 0.8392\n",
      "Epoch 35/120\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 0.7590 - val_loss: 1.0657\n",
      "Epoch 36/120\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 0.7596 - val_loss: 0.6495\n",
      "Epoch 37/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.7429 - val_loss: 0.6409\n",
      "Epoch 38/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 0.7455 - val_loss: 1.0398\n",
      "Epoch 39/120\n",
      "26150/26150 [==============================] - 10s 389us/step - loss: 0.7359 - val_loss: 0.8303\n",
      "Epoch 40/120\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 0.7400 - val_loss: 0.6136\n",
      "Epoch 41/120\n",
      "26150/26150 [==============================] - 11s 411us/step - loss: 0.7299 - val_loss: 0.8719\n",
      "Epoch 42/120\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 0.7196 - val_loss: 1.0129\n",
      "Epoch 43/120\n",
      "26150/26150 [==============================] - 10s 386us/step - loss: 0.7191 - val_loss: 0.5741\n",
      "Epoch 44/120\n",
      "26150/26150 [==============================] - 10s 388us/step - loss: 0.7061 - val_loss: 1.2454\n",
      "Epoch 45/120\n",
      "26150/26150 [==============================] - 11s 409us/step - loss: 0.7007 - val_loss: 1.1701\n",
      "Epoch 46/120\n",
      "26150/26150 [==============================] - 10s 392us/step - loss: 0.7057 - val_loss: 0.6512\n",
      "Epoch 47/120\n",
      "26150/26150 [==============================] - 11s 438us/step - loss: 0.7037 - val_loss: 0.4552\n",
      "Epoch 48/120\n",
      "26150/26150 [==============================] - 11s 434us/step - loss: 0.6899 - val_loss: 0.6117\n",
      "Epoch 49/120\n",
      "26150/26150 [==============================] - 11s 435us/step - loss: 0.6884 - val_loss: 0.7910\n",
      "Epoch 50/120\n",
      "26150/26150 [==============================] - 11s 416us/step - loss: 0.6902 - val_loss: 0.8243\n",
      "Epoch 51/120\n",
      "26150/26150 [==============================] - 11s 402us/step - loss: 0.6771 - val_loss: 0.7562\n",
      "Epoch 52/120\n",
      "26150/26150 [==============================] - 11s 403us/step - loss: 0.6742 - val_loss: 0.5622\n",
      "Epoch 53/120\n",
      "26150/26150 [==============================] - 10s 394us/step - loss: 0.6701 - val_loss: 0.6550\n",
      "Epoch 54/120\n",
      "26150/26150 [==============================] - 10s 398us/step - loss: 0.6684 - val_loss: 0.5891\n",
      "Epoch 55/120\n",
      "26150/26150 [==============================] - 10s 395us/step - loss: 0.6634 - val_loss: 1.0849\n",
      "Epoch 56/120\n",
      "26150/26150 [==============================] - 10s 393us/step - loss: 0.6558 - val_loss: 0.5241\n",
      "Epoch 57/120\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 0.6489 - val_loss: 0.6922\n",
      "Epoch 58/120\n",
      "26150/26150 [==============================] - 10s 375us/step - loss: 0.6472 - val_loss: 0.4730\n",
      "Epoch 59/120\n",
      "26150/26150 [==============================] - 10s 371us/step - loss: 0.6417 - val_loss: 0.5696\n",
      "Epoch 60/120\n",
      "26150/26150 [==============================] - 10s 373us/step - loss: 0.6450 - val_loss: 0.6671\n",
      "Epoch 61/120\n",
      "26150/26150 [==============================] - 10s 371us/step - loss: 0.6345 - val_loss: 0.9184\n",
      "Epoch 62/120\n",
      "26150/26150 [==============================] - 10s 375us/step - loss: 0.6354 - val_loss: 1.1623\n",
      "Epoch 63/120\n",
      "26150/26150 [==============================] - 10s 374us/step - loss: 0.6335 - val_loss: 0.6455\n",
      "Epoch 64/120\n",
      "26150/26150 [==============================] - 10s 372us/step - loss: 0.6265 - val_loss: 1.2424\n",
      "Epoch 65/120\n",
      "26150/26150 [==============================] - 10s 373us/step - loss: 0.6220 - val_loss: 0.3890\n",
      "Epoch 66/120\n",
      "26150/26150 [==============================] - 10s 387us/step - loss: 0.6299 - val_loss: 0.4942\n",
      "Epoch 67/120\n",
      "26150/26150 [==============================] - 10s 377us/step - loss: 0.6160 - val_loss: 0.4132\n",
      "Epoch 68/120\n",
      "26150/26150 [==============================] - 10s 372us/step - loss: 0.6142 - val_loss: 0.4353\n",
      "Epoch 69/120\n",
      "26150/26150 [==============================] - 10s 372us/step - loss: 0.6153 - val_loss: 1.3221\n",
      "Epoch 70/120\n",
      "26150/26150 [==============================] - 10s 375us/step - loss: 0.6073 - val_loss: 1.3662\n",
      "Epoch 71/120\n",
      "26150/26150 [==============================] - 10s 370us/step - loss: 0.6091 - val_loss: 0.5386\n",
      "Epoch 72/120\n",
      "26150/26150 [==============================] - 10s 371us/step - loss: 0.6119 - val_loss: 0.8472\n",
      "Epoch 73/120\n",
      "26150/26150 [==============================] - 10s 372us/step - loss: 0.6001 - val_loss: 0.5120\n",
      "Epoch 74/120\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 0.6069 - val_loss: 0.5337\n",
      "Epoch 75/120\n",
      "26150/26150 [==============================] - 10s 376us/step - loss: 0.5937 - val_loss: 0.7605\n",
      "Epoch 76/120\n",
      "26150/26150 [==============================] - 10s 375us/step - loss: 0.6017 - val_loss: 0.4993\n",
      "Epoch 77/120\n",
      "26150/26150 [==============================] - 10s 373us/step - loss: 0.5986 - val_loss: 0.6399\n",
      "Epoch 78/120\n",
      "26150/26150 [==============================] - 10s 373us/step - loss: 0.5945 - val_loss: 0.6436\n",
      "Epoch 79/120\n",
      "26150/26150 [==============================] - 10s 370us/step - loss: 0.5974 - val_loss: 0.6367\n",
      "Epoch 80/120\n",
      "26150/26150 [==============================] - 10s 371us/step - loss: 0.5918 - val_loss: 0.6394\n",
      "Epoch 81/120\n",
      "26150/26150 [==============================] - 10s 373us/step - loss: 0.5923 - val_loss: 0.6184\n",
      "Epoch 82/120\n",
      "26150/26150 [==============================] - 10s 370us/step - loss: 0.5882 - val_loss: 0.7724\n",
      "Epoch 83/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.5964 - val_loss: 0.4844\n",
      "Epoch 84/120\n",
      "26150/26150 [==============================] - 10s 377us/step - loss: 0.5917 - val_loss: 0.4562\n",
      "Epoch 85/120\n",
      "26150/26150 [==============================] - 10s 372us/step - loss: 0.5887 - val_loss: 0.8464\n",
      "Epoch 86/120\n",
      "26150/26150 [==============================] - 10s 371us/step - loss: 0.5826 - val_loss: 0.5131\n",
      "Epoch 87/120\n",
      "26150/26150 [==============================] - 10s 370us/step - loss: 0.5825 - val_loss: 0.8122\n",
      "Epoch 88/120\n",
      "26150/26150 [==============================] - 10s 373us/step - loss: 0.5785 - val_loss: 0.6596\n",
      "Epoch 89/120\n",
      "26150/26150 [==============================] - 10s 370us/step - loss: 0.5814 - val_loss: 0.4125\n",
      "Epoch 90/120\n",
      "26150/26150 [==============================] - 10s 375us/step - loss: 0.5769 - val_loss: 0.3443\n",
      "Epoch 91/120\n",
      "26150/26150 [==============================] - 10s 398us/step - loss: 0.5758 - val_loss: 0.5613\n",
      "Epoch 92/120\n",
      "26150/26150 [==============================] - 10s 393us/step - loss: 0.5744 - val_loss: 0.5614\n",
      "Epoch 93/120\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 0.5713 - val_loss: 0.5000\n",
      "Epoch 94/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5793 - val_loss: 0.7929\n",
      "Epoch 95/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5722 - val_loss: 0.7124\n",
      "Epoch 96/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5670 - val_loss: 0.7292\n",
      "Epoch 97/120\n",
      "26150/26150 [==============================] - 10s 383us/step - loss: 0.5686 - val_loss: 0.5490\n",
      "Epoch 98/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 0.5649 - val_loss: 0.5163\n",
      "Epoch 99/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5665 - val_loss: 0.8136\n",
      "Epoch 100/120\n",
      "26150/26150 [==============================] - 11s 427us/step - loss: 0.5602 - val_loss: 0.4597\n",
      "Epoch 101/120\n",
      "26150/26150 [==============================] - 12s 464us/step - loss: 0.5610 - val_loss: 0.6862\n",
      "Epoch 102/120\n",
      "26150/26150 [==============================] - 11s 428us/step - loss: 0.5532 - val_loss: 0.7252\n",
      "Epoch 103/120\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 0.5662 - val_loss: 0.4986\n",
      "Epoch 104/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 0.5555 - val_loss: 0.5336\n",
      "Epoch 105/120\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 0.5559 - val_loss: 0.5245\n",
      "Epoch 106/120\n",
      "26150/26150 [==============================] - 11s 409us/step - loss: 0.5551 - val_loss: 0.4932\n",
      "Epoch 107/120\n",
      "26150/26150 [==============================] - 10s 384us/step - loss: 0.5451 - val_loss: 0.4819\n",
      "Epoch 108/120\n",
      "26150/26150 [==============================] - 10s 382us/step - loss: 0.5571 - val_loss: 0.3812\n",
      "Epoch 109/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5493 - val_loss: 0.7608\n",
      "Epoch 110/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5515 - val_loss: 0.3206\n",
      "Epoch 111/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.5525 - val_loss: 0.4576\n",
      "Epoch 112/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 0.5448 - val_loss: 0.4875\n",
      "Epoch 113/120\n",
      "26150/26150 [==============================] - 10s 379us/step - loss: 0.5388 - val_loss: 0.4041\n",
      "Epoch 114/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5459 - val_loss: 0.9995\n",
      "Epoch 115/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5444 - val_loss: 0.3939\n",
      "Epoch 116/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5426 - val_loss: 0.4416\n",
      "Epoch 117/120\n",
      "26150/26150 [==============================] - 10s 380us/step - loss: 0.5343 - val_loss: 0.5255\n",
      "Epoch 118/120\n",
      "26150/26150 [==============================] - 10s 381us/step - loss: 0.5347 - val_loss: 0.6186\n",
      "Epoch 119/120\n",
      "26150/26150 [==============================] - 10s 385us/step - loss: 0.5381 - val_loss: 0.3860\n",
      "Epoch 120/120\n",
      "26150/26150 [==============================] - 10s 394us/step - loss: 0.5344 - val_loss: 0.5118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x253eb54b370>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_input, train_output2,\n",
    "          validation_data=(val_input,val_output2) ,epochs= 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59982018",
   "metadata": {},
   "source": [
    "#### evaluating the model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf0ef880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125/3125 [==============================] - 1s 282us/step - loss: 0.5109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5109319686889648"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(test_input, test_output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec3fedd",
   "metadata": {},
   "source": [
    "#### predicting the outcomes of model2 over a small data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d36c873f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 21.16051 ]\n",
      " [ 97.69935 ]\n",
      " [143.62082 ]\n",
      " [159.78867 ]\n",
      " [147.71585 ]\n",
      " [146.0476  ]\n",
      " [170.47035 ]\n",
      " [135.7854  ]\n",
      " [ 55.195213]\n",
      " [137.12183 ]]\n",
      "tf.Tensor(\n",
      "[[ 21.25]\n",
      " [ 98.25]\n",
      " [143.5 ]\n",
      " [160.25]\n",
      " [148.  ]\n",
      " [146.5 ]\n",
      " [170.75]\n",
      " [136.  ]\n",
      " [ 55.25]\n",
      " [137.25]], shape=(10, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "test = model2.predict(test_input_short)\n",
    "print(test)\n",
    "print(test_output_short2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1aa2ce",
   "metadata": {},
   "source": [
    "\n",
    "### saving the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "37c6c2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('parameters_model1.h5')\n",
    "model2.save('parameters_model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51532dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
